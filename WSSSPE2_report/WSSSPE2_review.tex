\documentclass[11pt, oneside]{amsart}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{color}
\usepackage{dcolumn}
\usepackage{float}
\usepackage{graphicx}
\usepackage[latin9]{inputenc}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{subfigure} 
\usepackage{psfrag}
\usepackage{tabularx}
\usepackage[hyphens]{url}
\usepackage{wrapfig}
\usepackage{longtable}
\usepackage{verbatim}

% The following three lines are used for displaying footnote in tables.
\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}


\usepackage{enumitem}
\setlist{leftmargin=7mm}
 
%\setcounter{secnumdepth}{3}
%\setcounter{tocdepth}{3}


\usepackage[bookmarks, bookmarksopen, bookmarksnumbered]{hyperref}
\usepackage[all]{hypcap}
\urlstyle{rm}

\definecolor{orange}{rgb}{1.0,0.3,0.0}
\definecolor{violet}{rgb}{0.75,0,1}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{cyan}{rgb}{0.2,0.7,0.7}
\definecolor{blueish}{rgb}{0.2,0.2,0.8}

\newcommand{\todo}[1]{{\color{blue}$\blacksquare$~\textsf{[TODO: #1]}}}
\newcommand{\note}[1]{ {\textcolor{blueish}    { ***Note:      #1 }}}
\newcommand{\katznote}[1]{ {\textcolor{magenta}    { ***Dan:      #1 }}}
\newcommand{\gabnote}[1]{ {\textcolor{cyan}    { ***Gabrielle:     #1 }}}
\newcommand{\nchnote}[1]{  {\textcolor{orange}      { ***Neil: #1 }}}
\newcommand{\manishnote}[1]{  {\textcolor{violet}     { ***Manish: #1 }}}
\newcommand{\davidnote}[1]{  {\textcolor{darkgreen}      { ***David: #1 }}}
\newcommand{\colinnote}[1]{ {\textcolor{red}    {***Colin: #1 }}}
\newcommand{\choinote}[1]{ {\textcolor{orange}    {***Choi: #1 }}}

% Don't use tt font for urls
\urlstyle{rm}

% 15 characters / 2.5 cm => 100 characters / line
% Using 11 pt => 94 characters / line
\setlength{\paperwidth}{216 mm}
% 6 lines / 2.5 cm => 55 lines / page
% Using 11pt => 48 lines / pages
\setlength{\paperheight}{279 mm}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
% You can use a baselinestretch of down to 0.9
\renewcommand{\baselinestretch}{0.96}

\sloppypar

\begin{document}

\title[]{Report on the Second Workshop on Sustainable Software for Science: Practice and Experiences (WSSSPE2)} 

\author{Daniel S. Katz$^{(1)}$, Sou-Cheng T. Choi$^{(2)}$, Nancy Wilkins-Diehr$^{(3)}$, Neil Chue Hong$^{(4)}$, 
\\James Howison$^{(5)}$, Miguel de Val-Borro$^{(6)}$, Richard Littauer$^{(7)}$, more TBD %by contributions 
\\{\scriptsize\today \quad  (Due:  $\backsim$ April 30)}} 
%\author{Gabrielle Allen$^{(11)}$,  \\
%Karen Cranston$^{(4)}$, Manish Parashar$^{(5)}$, David Proctor$^{(6)}$, \\
%Matthew Turk$^{(2)}$, Colin C. Venters$^{(7)}$,  Miguel de Val-Borro$^{(10)}$}
%
\thanks{{}$^{(1)}$ Computation Institute, University of Chicago \& Argonne National Laboratory, Chicago, IL, USA; \url{dsk@uchicago.edu}}
%
\thanks{{}$^{(2)}$ NORC at the University of Chicago and Illinois Institute of Technology, Chicago, IL, USA; \url{sctchoi@uchicago.edu}}
%
\thanks{{}$^{(3)}$ University of California-San Diego, San Diego, CA, USA; \url{wilkinsn@sdsc.edu}}
%
\thanks{{}$^{(4)}$ Software Sustainability Institute, University of Edinburgh, Edinburgh, UK; \url{N.ChueHong@software.ac.uk}}
%
\thanks{{}$^{(5)}$ University of Texas at Austin, Austin, TX, USA; \url{jhowison@ischool.utexas.edu}}
%
\thanks{{}$^{(6)}$ Princeton University, Princeton, NJ, USA; \url{todo}}  
%
\thanks{{}$^{(7)}$ TODO; \url{todo}}   
%  
%\thanks{{}$^{(4)}$ National Evolutionary Synthesis Center, Durham, NC, USA}
%
%\thanks{{}$^{(5)}$ Rutgers Discovery Informatics Institute, Rutgers University, New Brunswick, NJ, USA}
%
%\thanks{{}$^{(6)}$ International Consortium of Research Staff Associations, Dublin, Ireland}
%
%\thanks{{}$^{(7)}$ University of Huddersfield, School of Computing and Engineering, Huddersfield, UK}
%
%\thanks{{}$^{(9)}$ Saarland University, Saarbr\"{u}cken, Germany}
%
%\thanks{{}$^{(10)}$ Department of Astrophysical Sciences, Princeton University, Princeton, NJ, USA}
%
%\thanks{{}$^{(11)}$ University of Illinois, Champaign, IL, USA}

\begin{abstract}      
This technical report records and discusses the Second Workshop on Sustainable
Software for Science: Practice and Experiences (WSSSPE2). The workshop used an
alternative submission and peer-review process, which led to a set of papers
divided across five topic areas: exploring sustainability; software development
experiences; credit \& incentives; reproducibility \& reuse \& sharing; and code
testing \& code review. The report includes a description of the submission and
review process, the workshop keynote presentations, a series of lightning talks,
a discussion on sustainability, and the five discussions from the topic areas.
The report also contains a description of potential actions that were proposed
in the five
discussions.
\end{abstract}


\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\katznote{example comment by Dan}
%
%\gabnote{example comment by Gabrielle}
%
%\nchnote{example comment by Neil}
%
%\manishnote{example comment by Manish}
%
%\davidnote{example comment by David}

\note{google doc of notes for reference: \url{http://tinyurl.com/q6ew45v}}
%https://docs.google.com/document/d/1-BxkYWDQ6nNNBXBStUL0xcKF9qCTlEALwf928J_MemI/edit?usp=sharing

The Second Workshop on Sustainable Software for Science: Practice and
Experiences
(WSSSPE2)\footnote{\url{http://wssspe.researchcomputing.org.uk/wssspe2/}} was
held on  16 November, 2014 in conjunction with the International
Conference for High Performance Computing, Networking, Storage and Analysis
(SC14)\footnote{\url{http://sc14.supercomputing.org}}. WSSSPE2 followed the
model of a general initial workshop,
WSSSPE1\footnote{\url{http://wssspe.researchcomputing.org.uk/wssspe1/}}~\cite{WSSSPE1-pre-report,WSSSPE1},
which co-occurred with SC13, and a focused workshop,
WSSSPE1.1\footnote{\url{http://wssspe.researchcomputing.org.uk/wssspe1-1/}},
which was organized in July 2014 jointly with the SciPy 
conference\footnote{\url{https://conference.scipy.org/scipy2014/participate/wssspe/}}.

Progress in scientific research is dependent on the quality and accessibility of
software at all levels. Hence it is critical to address challenges related to
the development, deployment, maintenance, and overall sustainability of reusable
software as well as education around software practices. These challenges can be
technological, policy based, organizational, and educational, and are of
interest to developers (the software community), users (science disciplines),
software-engineering researchers, and researchers studying the conduct of
science (science of team science, science of organizations, science of science
and innovation policy, and social science communities). The WSSSPE1 workshop
engaged the broad scientific community to identify challenges and best practices
in areas of interest to creating sustainable scientific software. WSSSPE2
invited the community to propose and discuss specific mechanisms to move towards
an imagined future practice for software development and usage in science and
engineering. The workshop included multiple mechanisms for participation,
encouraged team building around solutions, and identified risky solutions with
potentially transformative outcomes. It strongly encouraged participation of
early-career scientists, postdoctoral researchers, and graduate students, with
funds provided to the conference organizers by the Moore Foundation and the
National Science Foundation (NSF), to support the travel of potential participants who
would not otherwise be able to attend, and young participants and those from
underrepresented groups, respectively.  These funds allowed \todo{number}
additional participants to attend, and each was offered the chance to present a
lightning talk.

This report extends a previous report that discussed the submission,
peer-review, and peer-grouping processes in detail~\cite{WSSSPE2-pre-report}.
It is also based on a collaborative set of notes taken with Google Docs 
during the workshop~\cite{WSSSPE2-google-notes}. Overall, the report discusses
the organization work done before the workshop (\S\ref{sec:preworkshop}); the
keynotes (\S\ref{sec:keynotes}); a series of lightning talks
(\S\ref{sec:lightning}), intended to give an opportunity for attendees to
quickly highlight an important issue or a potential solution; a session on
defining sustainability (\S\ref{sec:defining}). The report also gives summaries
of action plans proposed by five breakout sessions, which explored in specific
areas including sustainability (\S\ref{sec:exploring}); software development
experiences (\S\ref{sec:devel}); credit \& incentives (\S\ref{sec:credit});
reproducibility, reuse, \& sharing (\S\ref{sec:reproduce}); code testing \& code
review (\S\ref{sec:code_testing}). Lastly, the report also includes some
conclusions (\S\ref{sec:conclusions}) and an incomplete list of attendees
(Appendix~\ref{sec:attendees}).

 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Submissions, Peer-Review, and Peer-Grouping} \label{sec:preworkshop}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\note{this section is taken from \cite{WSSSPE2-pre-report}. It could be shortened.}

WSSSPE2 began with a call for papers~\cite{WSSSPE2-pre-report}. Based on the
goal of encouraging a wide range of submissions from those involved in software
practice, ranging from initial thoughts and partial studies to mature
deployments, but focusing on papers that are intended to lead to changes, the
organizers wanted to make submission as easy as possible. The call for papers
stated:

\begin{quote} We invite short (4-page) \textbf{actionable} papers that will lead
to improvements for sustainable software science. These papers could be a call
to action, or could provide position or experience reports on sustainable
software activities. The papers will be used by the organizing committee to
design sessions that will be highly interactive and targeted towards
facilitating action. Submitted papers should be archived by a third-party
service that provides DOIs. We encourage submitters to license their papers
under a Creative Commons license that encourages sharing and remixing, as we
will combine ideas (with attribution) into the outcomes of the workshop.
\end{quote}

The call included the following areas of interest:
\begin{quote}
\begin{itemize} 
\renewcommand{\labelenumi}{\textbf{\theenumi}.}
\setlength{\rightmargin}{1em}
\item defining software sustainability in the context of science and engineering
software
\begin{itemize}
\item how to evaluate software sustainability
\end{itemize}

\item improving the development process that leads to new software
\begin{itemize}
\item methods to develop sustainable software from the outset
\item effective approaches to reusable software created as a by-product of
research
\item impact of computer science research on the development of scientific
software
\end{itemize}

\item recommendations for the support and maintenance of existing software
\begin{itemize}
\item software engineering best practices
\item governance, business, and sustainability models
\item the role, operation, and
sustainability of community software repositories 
\item reproducibility, transparency needs that may be unique to science
\end{itemize}

\item successful open source software implementations
\begin{itemize}
\item incentives for using and contributing to open source software
\item transitioning users into contributing developers
\end{itemize}

\item building large and engaged user communities
\begin{itemize}
\item developing strong advocates
\item measurement of usage and impact
\end{itemize}

\item encouraging industry's role in sustainability
\begin{itemize}
\item engagement of industry with volunteer communities
\item incentives for industry
\item incentives for community to contribute to industry-driven projects
\end{itemize}

\item recommending policy changes
\begin{itemize}
\item software credit, attribution, incentive, and reward
\item issues related to multiple organizations and multiple countries, such as
intellectual property, licensing, etc.
\item mechanisms and venues for publishing software, and the role of publishers
\end{itemize}

\item improving education and training
\begin{itemize}
\item best practices for providing graduate students and postdoctoral
researchers in domain communities with sufficient training in software
development
\item novel uses of sustainable software in education (K-20)
\item case studies from students on issues around software development in the
undergraduate or graduate curricula
\end{itemize}

\item careers and profession
\begin{itemize}
\item successful examples of career paths for developers
\item institutional changes to support sustainable software such as promotion
and tenure metrics, job categories, etc.
\end{itemize}

\end{itemize}

\end{quote}


31 submissions were received; all but one used  
arXiv\footnote{\url{http://arxiv.org}} or
figshare\footnote{\url{http://figshare.com}} to self-publish their papers.

The review process was fairly standard. First, reviewers bid for papers. Then an
automated system matched the bids to determine assignments. After the reviewers 
completed their assigned reviews (with an average of 4.9 reviews per paper and
4.1 reviews per reviewer), they used
EasyChair\footnote{http://easychair.org/} to record
scores on relevance and comments. Finally, the organizers accessed the information to
decide which papers to associate with the workshop and provided the comments to
the authors to help them improve their papers.

The organizers decided to list 28 of the papers as significantly contributing to
the workshop, a very high acceptance rate, but one that is reasonable, given the
goal of broad participation and the fact that the reports were already
self-published.

WSSSPE1 was organized into sessions, each of which was aimed at discussing one
or more of the themes from the call for papers, with a few paper authors invited
to summarize the other papers in them as a panel, followed by general
discussion about that theme. The mapping of papers to themes was done by the
organizers.

For WSSSPE2, the organizers wanted to increase the interactivity of the
sessions, and to open the process of creating the sessions to the full program
committee, the paper authors, and others who might attend the workshop. In order
to do this, the organizers decided to use a breakout format for two sessions,
and to use an open process to determine the breakout topics. Specifically, Well
Sorted\footnote{\url{http://www.well-sorted.org}} was used in the following
steps:
\begin{enumerate}
\item Authors were asked to create Well Sorted ``cards'' for the papers. These
cards have a title (50 characters maximum) and a body (255 characters maximum).
\item Authors, program committee members, and members of the WSSSPE mailing list
were asked to sort the cards. Each person drags the cards, one by one, into
groups. A group can have as many cards as the person wants it to have, and it
can have whatever meaning that makes sense to that person.
\item Well Sorted  produces a set of averages of all the sorts, with various
numbers of card clusters.
\end{enumerate}

The organizers then chose a sort that contained five groups and that felt most
meaningful. After that, they decided on themes for the five groups, namely:
\begin{itemize}
\item Exploring Sustainability
\item Software Development Experiences
\item Credit \& Incentives
\item Reproducibility \& Reuse \& Sharing
\item Code Testing \& Code Review.
\end{itemize}

Finally, since some of the papers were not represented by cards in the process,
they were not placed in groups by the peer-grouping system; the authors of
these papers were asked which groups seemed the best for their papers---these
papers were then placed in those groups. Sections~\ref{sec:exploring}-\ref{sec:code_testing}
discuss the breakout groups, including a list of the papers associated with each
group.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Keynotes} \label{sec:keynotes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\note{Lead: Choi. \href{http://tinyurl.com/q6ew45v}{Google Notes}.
%https://docs.google.com/document/d/1-BxkYWDQ6nNNBXBStUL0xcKF9qCTlEALwf928J_MemI/edit?usp=sharing
\href{http://tinyurl.com/mnenzms}{Abstracts and slides}}
\end{comment}

The workshop featured two keynote addresses. In the opening keynote
presentation, Kaitlin Thaney of the Mozilla Science Lab talked about her
organization's work and policy to enable and support sustainable and
reproducible scientific research through the open web. The second keynote
speaker was Neil Chue Hong of Software Sustainability Institute. He shined a
light on how scientific software is prevalently driving advances in many science
and engineering fields. Both keynote speeches spawned further discussion among
workshop participants on the crucial notion \emph{sustainability} in the theme of
our workshop.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Kaitlin Thaney, Designing for Truth, Scale, and Sustainability~\cite{Thaney_slides}}
\label{keynote1}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Kaitlin Thaney is the Director of the Mozilla Science Lab (hereafter Mozilla).
Mozilla is a non-profit organization interested in openness, in news, in website
creation, and in Science, all taking advantage of the open web.

Thaney started noting the unfortunate fact that many current systems suffer the
unintended consequence of creating friction that hinders users, despite
designers' original purposes to do good. An example is the National Cancer
Institute's caBIG. A total of \$350 million was spent, including more than \$60
million for management. More than $70$ tools were created, but caBIG is still
seen as a failure\footnote{\url{http://tinyurl.com/maf6dz2}}. Those that had the
least investment were the most used; the most invested software were the least
utilized.

\begin{comment}
(Kaitlin didn't really say why? Were the tools used? Did they immediately
bit-rot? Was their development unsustained?)-should ask question about this. Is
there a caBIG report available? Quotes from Andrea Califano/Joe Gray, looks like
there is: \url{tinyurl.com/qdodbo7}.
%http://www.informationweek.com/architecture/report-blasts-problem-plagued-cancer-research-grid/d/d-id/1097068

What do we mean by open research? In regard to community/technology/practices

Inefficiency cartoon:  \url{http://www.xkcd.com/1445/}. 
\end{comment}

Thaney emphasized that for efficient reproducible open research, we would need
research tools (e.g., software repositories), social capital (e.g., incentives),
and capacity (e.g., training and mentorship). Our systems would need to
communicate with each other. A point was made by a member of the audience that
as systems become less monolithic, it often becomes harder to sustain the links
between them\footnote{See, for example, \url{http://tinyurl.com/l76tba2}.}.
%http://www.slideshare.net/jameshowison/scientific-software-sustainability-and-ecosystem-complexity
%(but does Anon Grizzly have other cites/info for that?).
%In contrast, works licensed by Creative Commons typically do not 
%have dependencies (a book, a photo, an artwork).

Thaney spoke about Mozilla's work around code citation, through a collaboration
and prototype crafted between Mozilla, GitHub, figshare, and Zenodo. This work
was presented at a closed meeting in May 2014 at the National Institutes of
Health (NIH) around these issues, sparking a conversation from that meeting
around what a \emph{Software Discovery
Index}\footnote{\url{http://softwarediscoveryindex.org/report/}} might look
like. The meeting included a number of publishers, researchers, and those behind
major scientific software efforts such as Bioconductor, Galaxy, and
nanoHUB. 
%facilitates more efficient scientific research. SDI identifies scientific
%software by archiving and standardizing metadata for software and hence help
%connect both developers and users.
Ted Habermann in the audience commented that if the metadata is minimal, it
would be less onerous for data providers, but more burdensome for users---it
could be challenging to keep a balance between what have to be captured and what
would be ideal if we do not want to lose user engagement as in the case of the
old Harvard Dataverse, finding often only the first four fields of three pages
of metadata were filled out.
 
The speaker concluded her talk urging the audience to design scientific software
with the general community, not an individual, in mind; and to design to unlock
latent potential of our systems. In addition, she encouraged everyone to rethink
how we reward researchers and support roles.
%Lastly, she cautioned the community to be mindful of jargon or semantics traps.


%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Neil Chue Hong, We are the 92$\%$~\cite{Hong_slides}}
\label{keynote2}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{comment}
In a recent survey of UK research-intensive universities, 92\% of researchers
said they used research software and 68\% said their research would be impossible
without software. Yet 71\% have had no formal software training, and few are
ready to apply many of the things we take for granted such as testing or
virtualization. WSSSPE represents the pinnacle of what we understand to be the
best practice around scientific software in our community. My talk will
challenge the workshop participants to come up with ways of taking this best
practice to those 92\% of researchers in a way that will lead to maximum benefit
to the scientific community.
 \end{comment}

%\url{http://dx.doi.org/10.6084/m9.figshare.1243288} 

Neil Chue Hong is the Director of the Software Sustainability Institute (SSI) in the 
United Kingdom. The SSI was founded to support the UK's research software 
community by cultivating better, more sustainable research software to enable
world-class research. Chue Hong's keynote started by making the point of
that the use of -- and reliance on -- software is pervasive in all areas of
world-leading research, showing examples from disciplines as diverse as
humanities and high-energy physics, and quoting Kersten Kleese Van Dam of the
Pacific Northwestern National Laboratory via a petition
campaign\footnote{https://www.change.org/p/everyone-in-the-research-community-we-must-accept-that-software-is-fundamental-to-research-or-we-will-lose-our-ability-to-make-groundbreaking-discoveries}
at change.org, ``\emph{Today there are very few science areas left which do not
rely on IT and thus software for the majority of their research work. More
importantly key scientific advances in experimental and observational science
would have been impossible without better software.}'' He also cited Daniel
Katz, Software Infrastructure for Sustained Innovation (SI2) Program Director of
the NSF, ``\emph{Scientific discovery and innovation are advancing along
fundamentally new pathways opened by development of increasingly sophisticated
software. Software is an integral enabler of computation, experiment and theory,
and directly responsible for increased scientific productivity and enhancement
of researchers' capabilities.}''

Chue Hong drew attention to the issue that in the cyber-infrastructure and
high-performance community, we all too often disregard the hundreds of thousands
of researchers developing software thinking they are the long tail. In fact, the
numbers point to the fact that they are actually the mainstream and we
(referring to people including the audience of the talk) are the elite minority.
He emphasized that software is no longer special; it is both essential to and
common in scientific research. A 2014
survey\footnote{http://www.software.ac.uk/blog/2014-12-04-its-impossible-conduct-research-without-software-say-7-out-10-uk-researchers}
conducted by the SSI polled researchers from 15 research-intensive UK
universities (406 respondents covering a representative range of funders,
disciplines, and seniority) reported that 92\% confirmed the use of research
software and 89\% affirmed that it would be impossible or difficult to conduct
research without software. Nevertheless the British research community is just
starting to understand the magnitude of the issue. Whilst many researchers make
use of software such as MATLAB, SPSS, and Excel, data from the aforementioned
SSI survey shows that over half (56\%) of respondents developed their own
research software (which equates to over 140,000 researchers if extrapolated
across the UK) and yet 71\% had no formal software-development training, having
to rely on their own coding skills.
 
Examining another aspect of the size of the research software community, 
Chue Hong noted that the costs of software-reliant research in the United Kingdom
included \textsterling 840 million of investment in the financial year
2013--2014, and this amount has risen by 3\% on average over the past four
years. About 30\% of total research investment has been spent on research that
relies on software over the last four financial years. These numbers stemmed
from an analysis by the SSI of data from $49,650$ grant titles and abstracts published on
Gateway to Research between years 2010 and 2014. A similar analysis of university 
jobs advertised in the same period discovered that despite this investment, only
4\% of positions were software development related, and of these only 17\% were
explicitly named as a software developer / software engineer position: the vast
majority being advertised as research associate / research assistant positions.
This in turn leads to the issue of career paths for those bridging the research
and software worlds, who are essential to support the use and further
development of research software, a point highlighted by a graphic showing UK
STEM graduate career paths\footnote{Source: The Scientific Century, Royal
Society, 2010 (revised to reflect first stage clarification from ``What Do PhD's
Do?'' study)} showing that only 3.5\% were able to secure permanent positions. 
 
To conclude, Chue Hong led the audience in discussing the following questions: 
What are \emph{we} going to \emph{do} to help and benefit the 92\% of researchers 
who rely on software? Who do we need to persuade? What are the incentives we 
need to put in place? Finally, he challenged the workshop participants to change 
the current deficient practices in research and academia.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lightning Talks} \label{sec:lightning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\note{Lead: Choi. Volunteers welcome.
\href{http://wssspe.researchcomputing.org.uk/wssspe2/lightning-talks/}{Slides.}}
\end{comment}
 
\begin{comment}
Lighting talks, breakout groups, working groups are all different. Lightning
talks were new in WSSSPE2. They were offered to all paper contributors, and some
other attendees, those who got travel awards. Breakout groups in WSSSPE1 were
just discussions about topics; in WSSSPE2, the breakout groups were supposed to
come up with plans for actions in specific areas. The idea of working groups is
that they will come out of breakout groups as a way for the groups to actually
do the actions over time, not at the workshops.
 
Email addresses of speakers:
gvwilson@software-carpentry.org, Colin Venters <C.Venters@hud.ac.uk>,
j.spencer@imperial.ac.uk, erinrobinson@esipfed.org, marpierc@iu.edu,
jwpeterson@gmail.com, abani@buffalo.edu, clenhardt@renci.org, dsk@uchicago.edu,
Samin.Ishtiaq@microsoft.com, james@howison.name, s.a.harris@leeds.ac.uk,
marcus.hanwell@kitware.com, thabermann@hdfgroup.org, rdowns@ciesin.columbia.edu,
cboettig@gmail.com, jakob.blomer@cern.ch, editor@ascl.net 
\end{comment}  
  
Lightning talks were a new feature in WSSSPE2. Since the workshop program was
mostly dedicated to discussions, the organizers wanted to give the attendees a
chance to also `make a pitch' for an idea, either representing a contributed paper
or something different.  Eighteen attendees volunteered to participate in the lightning
talks, each given only two minutes to speak and at most one slide to show.
The talks were presented in reverse alphabetic order of speakers' last names. 
In the rest of this section, we highlight the gist of some of the speakers' messages.

 
%\item Greg Wilson: Close Enough for Scientific Work

\subsection{Colin Venters: The Nebuchadnezzar Effect: Dreaming of Sustainable Software
through Sustainable Software Architectures~\cite{Venters_poster}}
Venters proposed that sustainable software is a composite, first-class,
non-functional requirement (NFR) that is at a minimum a measure of a system's
maintainability and extensibility, but may also include other NFRs such as
efficiency (e.g., energy, cost), interoperability, portability, reusability,
scalability, and usability. To achieve technically sustainable software, Venters
suggested that software architectures are fundamental as they are the primary
carrier of system NFRs, i.e., pre-system understanding; and influence how
developers are able to understand, analyze, extend, test, and maintain a
software system, i.e., post-deployment system understanding. In addition,
Venters highlighted that sustainability of software architectures needs to be
addressed to endure different types of change and evolution in order to mitigate
architectural drift, erosion, and knowledge vaporization.

%\item James Spencer: Developing New Developers

%\item Erin Robinson:  Sustaining Science Bazaars

\subsection{Marlon Pierce: Patching It Up, Pulling It Forward~\cite{Pierce_poster}}
Pierce discussed how open open source is. Open software needs a diverse, openly
governed community behind it, just as it needs open licensing and a public code
repository. To probe the level of governance within open source projects, he and
his co-authors (Suresh Marru and Chris Mattmann) suggested a contest to encourage
individual developers to submit patches and pull request to projects that are
important to them. This simple mechanism will expose several governance
mechanisms, such as how easy it is for independent developers to communicate
with project leadership, how projects accept and properly license third-party
contributions, and how projects make decisions such as granting source tree
write access.

\subsection{John Peterson: Continuous Integration for Concurrent MOOSE Framework and
Application Development on GitHub~\cite{Peterson_poster}} 
%Affiliation: Idaho National Laboratory
Peterson from the Idaho National Laboratory reported that in March 2014, the
MOOSE framework was released under an open source license on GitHub,
significantly expanding and diversifying the pool of current active and
potential future contributors on the project. The MOOSE team employs an
extensive continuous integration test suite to ensure that both the framework
and the applications based on the framework are verified before any code changes
are accepted into the repository. They use a combination of built-in Git
features such as branching and submodules, GitHub API integration capabilities,
and in-house developed testing software to perform this verification and update
the dependent applications in a relatively seamless manner for users.

\subsection{Abani Patra: Execute it~\cite{Patra_poster}}
%Univ at Buffalo, SUNY
Patra discussed the value of
an available and easily accessible platform for executing scientific software,
e.g., HUBzero, to access XSEDE or other computing resources.
Such a platform for executing benchmark problems (even at small
scale) allows the developer community access to a reference implementation and
provides an easy way to train the larger user community. A second idea of this
presentation was that for true usability, much more attention and support needs to be given to the
integrated use of simulation tools inside complex workflows.
%needs supporting the user community workflow. 

%\item Christopher Lenhardt: Open Science for Synthesis (OSS): Filling the Gap in
%Early Career Training

\subsection{Daniel Katz: Implementing Transitive Credit with JSON-LD~\cite{Katz_transitive_credit_poster}}
Science and engineering research increasingly relies on activities that
facilitate research but are not rewarded or recognized, such as: data
sharing; developing common data resources, software and methodologies; and
annotating data and publications. To promote and advance these activities, we
must develop mechanisms for assigning credit, facilitate the appropriate
attribution of research outcomes, devise incentives for activities that
facilitate research, and allocate funds to maximize return on investment. Katz
discussed the issue of assigning credit for both direct and indirect
contributions by using JSON-LD to implement a prototype transitive credit
system.

\subsection{Samin Ishtiaq: Daemons, Notifications and Sustaining Software}%~\cite{Ishtiaq_poster}}
%  Microsoft Research Cambridge
The reproduction and replication of novel results has become a major issue in
computer science, systems biology, and other computational disciplines. These
include both the inability to re-implement novel algorithms and approaches, and
lack of an agreement on how and what to benchmark these algorithms on.
Ishtiaq from Microsoft Research Cambridge pointed out these problems and 
made several suggestions to address them.

\subsection{James Howison: Retract all Bit-Rotten Publications}%~\cite{Howison_poster}}
Howison sought to provoke discussion by proposing that papers whose workflows are
not kept current with the changing software ecosystem should be automatically
retracted. This would create an incentive for authors to keep their
software current and usable, rather than the current situation in which every
potential user has to do this individually. A softer version of the proposal
would identify papers whose software workflow has become bit-rotten and allow
others to keep the code up to date, either adding them as new authors of the
paper or providing credit in some other form for their academic service.

%\item Sarah Harris: Sustainability of Multidisciplinary Software: A Science
%Perspective

%\item Marcus D. Hanwell: We Need Tool Builders for Sustainable Scientific
%Software

%\item Ted Habermann: Communities as the Whole Product

\subsection{Robert Downs: Community Recommendations for Improving Sustainable
Scientific Software Practices~\cite{Downs_poster}}
Robert Downs, of the Columbia University Center for International Earth
Science Information Network (CIESIN), 
%presented Community Recommendations for
%Improving Sustainable Scientific Software Practices (2014), and 
described a
focus group study conducted with %W. Christopher Lenhardt, Erin Robinson, Ethan
% Davis, and Nicholas Weber of 
the Science Software Cluster (SSC) of the
Federation of Earth Science Information Partners (ESIP). For the study, almost
300 attendees of the 2014 Summer ESIP Meeting were invited to participate in
simultaneous roundtable discussions on sustainability of science software. Over
two-thirds of the roundtable focus groups responded to a semi-structured survey
that contained three sets of questions eliciting recommendations for near-term
actions of the community to improve sustainable software practices. Initial
analysis of the
participants' responses to the questionnaire revealed several suggestions, which
included improving community engagement and collaborative activities, increasing
understanding and awareness, and creating incentives to motivate sustainable
science software practices.
%Based on the recommendations
%offered by the focus group respondents for near-term actions, 
The ESIP SSC plans to engage the community %to achieve progress in the near future 
in the recommended activities for improving sustainable scientific
software practices.

\begin{comment}
Downs RR, Lenhardt WC, Robinson E, Davis E, Weber N. 2014. Community
Recommendations for Improving Sustainable Scientific Software Practices. 2nd
Workshop on Sustainable Software for Science: Practice and Experiences
(WSSSPE2), New Orleans, Louisiana, 16 November 2014.
http://wssspe.researchcomputing.org.uk/wp-content/uploads/2014/11/Downs.pdf and
http://dx.doi.org/10.7916/D8Q52NBC
\end{comment}

\subsection{Carl Boettiger: rOpenSci: Building Sustainable Software by Fostering a
Diverse Community~\cite{Boettiger_poster}}
Boettiger described how the rOpenSci project has been successful by focusing not
just on building software but also on building a community of researchers who
learn and adopt their approaches to reproducible research and sustainable
software practice. Through outreach, mentoring, workshops, and hackathons, they
have not only reached new users, but also turned users into co-developers of
robust software and good practices to support data science research across a
growing set of disciplines.

\subsection{Jakob Blomer: The Need for a Versioned Data Analysis Environment~\cite{Blomer_poster}}
%
Large-scale scientific endeavors, such as the discovery of the Higgs
boson at the Large Hadron Collider (LHC), often rely on complex software
stacks.  Maintaining thousands of dependencies of software
libraries and operating system versions has shown that despite
source code availability, the setup and the validation of a minimal
usable analysis environment can easily become prohibitively expensive.
%There is a substantial gap between merely having access to open source
%software and the ability to create a data analysis runtime environment.
 In high-energy physics, a
special purpose, open-source, versioning and snapshotting file system
(CernVM-FS) that is used to capture and distribute entire software stacks,
providing instant access to ready-to-run data analysis environments,
proved to be very useful.
% for the global scientific community as well as for
%interested citizens, for instance through the CERN Open Data Portal
%(http://opendata.cern.ch). \todo{revise}
 

\subsection{Alice Allen: Find it! Cite it!}%~\cite{Allen_poster}}
%
The Astrophysics Source Code Library (ASCL) is an online registry of
scientist-written software used in astronomy research. Their primary interest is
rendering research more transparent by making this software more discoverable for
examination. The ASCL is treated as a publication by an indexing resource for
astronomy, the Astrophysics Data System (ADS). ADS tracks citations to what it
indexes, including citations to ASCL entries. Increasing rewards for writing
software, whether through citation, transitive credit or other methods, gives
software authors a powerful reason to take the time to build sustainability into
their software and is an excellent way to drive community change. 

\begin{comment}
The ASCL actively looks for opportunities to use change management strategies to
move astronomy to a more transparent discipline; these strategies can be used in
other disciplines to further change in them.
\end{comment}
 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Defining Sustainability} \label{sec:defining}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo{writing led by Dan - other contributions welcome}

In the first interactive session, the attendees divided themselves into groups
to discuss software sustainability. They were asked to
\begin{enumerate}
\item discuss what the term ``software sustainability'' meant to them

\item determine three things they considered to be significant enablers of
software sustainability

\item determine three things they considered to be significant barriers to
software sustainability
\end{enumerate}
Once each group had come up with answers, all the answers were compiled, and the
attendees voted on which they thought were important by a show of hands.

The general responses to what software sustainability meant were:
\begin{itemize}
\item keeping software scientifically useful
\item separating techniques in code from knowledge in code
\item that an adequately large community finds value in software and is willing
to sustain it.
\end{itemize}
\katznote{any discussion of this?}

The enablers of and barriers to software sustainability, roughly ranked by
attendee votes are shown in
{\tablename}s~\ref{tb:software_sustainability_enablers} and
\ref{tb:software_sustainability_barriers}, respectively.\footnote{A few other
items were suggested as barriers, but were not voted on due to lack of time in
the session:
layering up dependencies;
using software past its sustainable life;
using software past its usable life;
inertia for accepted answers versus wrong or right answers;
monolithic or poor code; and
need to restructure code when hardware/software/libraries change.
}
\katznote{any discussion of these tables?}

\begin{table*}[ht]
 \centering
\caption{Enablers of software sustainability, with 0 to 10 `*'s roughly
indicating the fraction of attendees who voted for an item as important.}
\label{tb:software_sustainability_enablers}
  \begin{scriptsize}
  \begin{tabular}{ | p{1.65cm} | p{10.0cm} |}
   %  \begin{tabular}{ | p{1.9cm} | p{11.5cm} |}
    \hline
  Importance & Item \\ \hline \hline
********** & healthy and vibrant communities; vibrant community to champion software \\ \hline
********** & designing for growth and extension -- open development \\ \hline
******* & culture in community for reuse \\ \hline
**** & portability \\ \hline
**** & culture in development community to support transition between developers \\ \hline
*** & interdisciplinary people: science + IT experience \\ \hline
** & planning for end of life \\ \hline
** & make smart choices about dependencies \\ \hline
* & thinking of software as product lines -- long term vs.~short term view \\ \hline
 & not all communities need new software \\ \hline
 & converting use into resources \\ \hline
    \end{tabular}
    \end{scriptsize}
\end{table*} 

\begin{table}[ht]
\caption{Barriers to software sustainability, with 0 to 10 `*'s roughly
indicating the fraction of attendees who voted for an item as important.}  
\label{tb:software_sustainability_barriers} 
 \centering
  \begin{scriptsize}
  \begin{tabular}{ | p{1.65cm} | p{10.0cm} |}
% \begin{tabular}{ | p{1.9cm} | p{11.5cm} |}
   \hline
  Importance & Item \\ \hline \hline
******* & lack of incentives, including promotion and tenure process; promotion
and tenure process in academic is incompatible with sustainability \\ \hline
*****  & absent or poor documentation \\ \hline
***** & funding to ensure sustainability is difficult to obtain \\ \hline
**** & developers are not computer scientists; don't have software engineering
practices (in particular, those needed to scale-up projects to support and be
developed by a large sustainable community) \\ \hline
*** & overreliance on one or two people -- the `bus test'\footnote{``Bus test''
in Table~\ref{tb:software_sustainability_barriers} refers to the smallest number
of key people a project would need to lose to become non-viable (the larger the
number, the healthier the project).} \\ \hline
** & rate of change of underlying technologies \\ \hline
** & lack of business models for sustainability \\ \hline
* & lack of training for how to build sustainability into the system \\ \hline
 & maintenance for software is not visible, appears to ``just happen'' \\ \hline
 & licensing issues \\ \hline
 & staff turnover -- lack of continuity \\ \hline
\end{tabular}
 \end{scriptsize}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exploring Sustainability} \label{sec:exploring}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{Colin to lead writing of this section.}

\note{\href{http://tinyurl.com/mpbhvyb}{Google doc notes}}
%https://docs.google.com/a/uchicago.edu/document/d/10XkshP3YjXnA5JQgP-UFHbOxshWczzehWGSo6V7bNH4/edit

\todo{short intro to group here}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Discussion and Actions}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Papers}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The papers that were discussed in the Exploring Sustainability group are:
\begin{itemize}
\item Mario Rosado de Souza, Robert Haines, and Caroline Jay. Defining
Sustainability through Developers' Eyes: Recommendations from an Interview
Study~\cite{wssspe2_rosada_de_souza}

\item Robert Downs, W. Christopher Lenhardt, Erin Robinson, Ethan Davis, and
Nicholas Weber. Community Recommendations for Sustainable Scientific
Software~\cite{wssspe2_downs}

\item Abani Patra, Matthew Jones, Steven Gallo, Kyle Marcus, and Tevfik Kosar.
Role of Online Platforms, Communications and Workflows in Developing Sustainable
Software for Science Communities~\cite{wssspe2_patra}

\item Marlon Pierce, Suresh Marru, and Chris Mattmann. {WSSSPE2}: Patching It Up,
Pulling It Forward~\cite{wssspe2_pierce}

\item Justin Shi. Seeking the Principles of Sustainable Software
Engineering~\cite{wssspe2_shi}

\item Colin C. Venters, Michael K. Griffiths, Violeta Holmes, Rupert R. Ward, and
David J. Cooke. The Nebuchadnezzar Effect: Dreaming of Sustainable Software
through Sustainable Software Architectures~\cite{wssspe2_venters}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Software Development Experiences} \label{sec:devel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{Nancy to lead writing of this section}

\note{\href{http://tinyurl.com/pn4eq8z}{Google doc notes}}
%https://docs.google.com/a/uchicago.edu/document/d/1_q0jmiEPNFRGAEtGLk_QxQ1vVokgTDYruTGURowNGkY/edit

Of the short actionable papers that would lead to improvements for sustainable
software science, 11 submissions were categorized in the  Software Development Experiences
group. Because of the large number, we split these into two subgroups
prior to the event. Some common themes helped in this division. For
example, several papers that addressed education and training issues,
including best practices and case studies, were grouped
together. Others discussed experiences with registries, developer
collectives and specific examples of successful, sustainable software
(including a valuable industry perspective).
 
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Discussion and Actions}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\katznote{need something about subgroup A here}

Subgroup B discussed four papers and had six
participants. Because of the nature of the papers, training emerged as a
common theme, however the conversation was wide-ranging, including
incentives, reproducibility and funding to promote sustainability.

\subsubsection{Subgroup A Discussion}

\subsubsection{Subgroup B Discussion}
The statement ``applied computer science is being attempted in academia
without any formal training'' kicked off our discussion. The group brought
expertise in several different training models, from a two-day Software
Carpentry workshop, to a three-week Open Science for Synthesis (OSS) program
to semester-long programs. We discussed the pros and cons of different
training approaches, also touching on informal learning, for example where
people learning the necessary skills because they had cross-disciplinary
people (``boundary scientists'') in their work environments of whom they
could ask questions.

Some papers explored gaps in training of early career scientists. Industry
participants in our group confirm this. We asked ourselves, ``Are
traditional courses failing?'' We think yes. Changes to undergraduate
curricula requirements are difficult. But as programming models become
more complex, we have to raise the skill levels. Skills must be improved
not only in traditional programming -- learning languages and algorithms --
but around professional software development. We want to get to a point
where ``Everyone has a new minimum now -- everyone knows Git.'' Developers
also need training on licensing choices. Just because a code is on Git
does not mean that it is open.

``How do you get people to look for the training they need?'' wondered one
participant. People seek out opportunities like Software Carpentry to
augment skills. While some instruction can be done in an institutional
curricula, independent groups (non-profits, institutes) have more
flexibility. Some asked whether Silicon Valley be interested in funding
training so that people are better prepared to enter the workforce? Some
felt that companies were reluctant to deliver training because employees
then leave the company. While others felt that pushing all training to
industry could lead to good technical people ``getting on the Google bus''
and leaving the sciences.

``How will we know when people are trained effectively in these new
skills?'' We discussed certification. It can be hard to build the
recognition of Java or database certifications amongst all technologies. OSS
offers badges to those completing training. However we need to demonstrate
proficiency, not just completion. Google Summer of Code is a big CV
augmentation for participants---can we duplicate something similar?

The group also discussed how to fund training. On participant observed
that NYU runs a six-week data-science training; companies grab the graduates and
pay for those they hire. OSS also used the NSF Software Institutes program
as a vehicle to fund training activities. Software Carpentry uses a
collaborative teaching approach where  people publish open teaching
materials and receive credit for their use.


We then discussed career paths for those supporting sustainable software.
While tenure track is not the only option for students, the challenges for
those who remain in academia can be large. Research scientists are
entirely dependent on soft money, which can be unpredictable. Postdocs and
those who do pursue tenure track positions need to publish and see no
rewards in software development. These challenges were all identified at
WSSSPE1. What actions would we recommend to improve things? Altmetrics and
download statistics may slowly change the system and improving a
developer's ability receive credit for time invested in software
development. NSF recognition of data products (including software) as well
as publications is also helping.


We asked ourselves, ``Where are there examples of things that are changing
because of this and how can we build momentum?'' One example demonstrates
change over time. In 2007, nanoHUB listed the academic reward structure as
a problem in an EDUCAUSE report where the authors note, ``In the future,
nanoHUB researchers are hoping to change the research culture. While they
recognize that young faculty members are unlikely to get tenure based on
their nanoHUB contributions, they hope to encourage faculty to think
beyond their own research needs to consider publishing tutorials and other
Web content in their field on the nanoHUB site.'' Fast forward seven years
later where, in a 2014 iSGTW article, quote nanoHUB Principal Investigator (PI) Gerhard Klimeck, ``A
former student of mine published eight tools on nanoHUB, serving over
6,000 people with his tools. He then joined a university as a professor
and introduced nanoHUB. Use of the gateway from that university
skyrocketed; he used nanoHUB in existing classes, created new classes, and
infused it in his research. Ultimately, the professor's department head
associates his two-year rise to tenure with the notoriety and innovation
he gained through nanoHUB.''  

The group also discussed ``attribution trees,'' an idea put forward by Dan
Katz and Arfon Smith where a chain of attributions can, for example, give
credit to those developing libraries and building blocks that other pieces
of software use. The group considered where to push the use of this, which
journals? On participant noted that Dryad works with the journals. If a
paper is accepted, the supporting data must be submitted. We discussed
reproducibility as a component of the journal review process and ``active
papers,'' with immediate links to the data and software.

We then discussed variations amongst scientific domain areas and wondered
``Are some communities more or less open than others?'' To some members of
the group biology seems to be open, while physics less so. Some felt with
the more recent development of bioinformatics as a field there were fewer
historical practices to undo. Physics has pre-print philosophy to
overcome. Environmental sciences may be mixed. The group felt the
biomedical area, however, was very competitive and closed. One participant
mentioned blueprints for going open source (like NWChem recently did)
where authors outline how this helps, what you do and what the next steps
are.


We then moved beyond our training discussion to address funding that
encourages sustainable software -- funding of both people and projects
that create a true system to support sustainability. We called this
``institutionalized serendipity.'' As science is increasingly reliant
on software, one participant observed that ``software development can have
much broader impact than publishing individual research, but it is not
viewed that way.'' Because of this centrality, one participant mentioned
that training ought to be called Science Carpentry rather than Software
Carpentry. We felt we were beginning to see changes in the research
community as a result of the NSF's data management plan requirements. PIs 
are thinking more about data and some university libraries are offering
data repositories. We wondered if a software management plan might be
effective. ``How might funding programs need to adapt to reward good
software development practices?'' we asked ourselves. We thought about
best practices such as version control, test harnesses, mailing lists, bug
tracking, community contribution, and reuse where appropriate. We thought
about measuring success through usage statistics (downloads, altmetrics).
``Should funders demand reproducibility?'' we wondered. In order for results
to be reproduced, software would need to be carefully curated.

Again, our industry participants contributed unique viewpoints. Partnering
with industry was seen as one path to sustainability. The unique partnerships
Kitware engages in promote academic freedom while creating an income
stream from certain portions of the software. This type of approach to
sustainable software frees researchers from performing tasks that do not
offer  the rewards their institutions value. We also discussed  about
successful models for industry partnerships that preserve open science.
Participants noted that there are some NSF programs that prohibit
partnerships with for-profit companies (but there are other programs in which this is
encouraged).


\subsubsection{Subgroup A Actions}

\subsubsection{Subgroup B Actions}

Subgroup B then focused on actions it could take. It discussed development
of a white paper that describes a matrix approach to training
(multi-day, multi-week, semester). The white paper might include a survey
of existing techniques. There are many, some dating back many years, for
example the Interuniversity Consortium for Political and Social Research
(ICPSR) and various summer institutes. The white paper could Include a
call for a comprehensive assessment of these techniques. We need to think
carefully about the right venue for such a white paper, where would it
have the most impact. The subgroup believes it would need to approach editors
directly to ascertain this.

The group felt that training in techniques that promote sustainability has
a range of benefits: career paths, educated reviewers, reproducible
science and more. There is some information on how that has been
approached and assessed, but more is needed. The group felt that this
training is undervalued and that it is important to communicate the return
on investment -- both individual ROI (skills that make scientists more
effective and more marketable) and funder ROI (better use of taxpayer
funds, research more likely to be reproducible because sustainable
software exists, better trained reviewers).



%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Papers}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The papers that were discussed in the Software Development Experiences Subgroup~A are:

\begin{itemize}
\item Michael R. Crusoe and C. Titus Brown. Channeling community contributions to
scientific software: a hackathon experience~\cite{wssspe2_crusoe}

\item Marcus Hanwell, Patrick O'Leary, and Bob O'Bara. Sustainable Software
Ecosystems: Software Engineers, Domain Scientists, and Engineers Collaborating
for Science~\cite{wssspe2_hanwell}

\item W. Christopher Lenhardt, Stanley Ahalt, Matt Jones, J. Aukema, S. Hampton,
S. R. Hespanh, R. Idaszak, and M. Schildhauer. {ISEES-WSSI} Lessons for
Sustainable Science Software from an Early Career Training Institute on Open
Science Synthesis~\cite{wssspe2_lenhardt}

\item Jory Schossau and Greg Wilson. Which Sustainable Software Practices Do
Scientists Find Most Useful?~\cite{wssspe2_schossau}

\end{itemize}

The papers that were discussed in the Software Development Experiences Subgroup~B are:

\begin{itemize}

\item Jordan Adams, Sai Nudurupati, Nicole Gasparini, Daniel Hobley, Eric
Hutton, Gregory Tucker, and Erkan Istanbulluoglu. Landlab: Sustainable Software
Development in Practice ~\cite{wssspe2_adams}

\item Alice Allen and Judy Schmidt. Looking before leaping: Creating a software
registry~\cite{wssspe2_allen}

\item Carl Boettiger, Ted Hart, Scott Chamberlain, and Karthik Ram. Building
software, building community: lessons from the {ROpenSci}
project~\cite{wssspe2_boettiger}

\item Yolanda Gil, Eunyoung Moon, and James Howison. No Science Software is an
Island: Collaborative Software Development Needs in
Geosciences~\cite{wssspe2_gil}

\item Ted Habermann, Andrew Collette, Steve Vincena, Werner Benger, Jay Jay
Billings, Matt Gerring, Konrad Hinsen, Pierre de Buyl, Mark K\"{o}nnecke, Filipe
Rnc Maia, and Suren Byna. The Hierarchical Data Format ({HDF}): A Foundation for
Sustainable Data and Software~\cite{wssspe2_habermann}

\item Eric Hutton, Mark Piper, Irina Overeem, Albert Kettner, and James Syvitski.
Building Sustainable Software - The {CSDMS} Approach~\cite{wssspe2_hutton}

\item James S. Spencer, Nicholas S. Blunt, William A. Vigor, Fionn D. Malone, W.
M. C. Foulkes, James J. Shepherd, and Alex J. W. Thom. The {H}ighly {A}ccurate
{N-DE}terminant ({HANDE}) quantum {Monte Carlo} project: Open-source stochastic
diagonalisation for quantum chemistry~\cite{wssspe2_spencer}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Credit \& Incentives} \label{sec:credit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{Dan leading writing of this section}

\note{\href{http://tinyurl.com/k8ruyn9}{Google doc notes}}
%https://docs.google.com/document/d/1Jsi_trBT5cjEXVnGjGHJpcIhMEcVpAn6XmYNxfIHXv0/edit#heading=h.gk1dzxu9wi6q

This group, with just three papers but a large amount of interest and
participation from attendees, focused on the institutional, social, and cultural
mechanisms that encourage the creation and maintenance of shared software, in the
context of what now exists but focusing on what mechanisms are desired and how
we might achieve them.


%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Discussion and Actions}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the first discussion session, this group decided to break into two smaller groups,
each independently working through the same general topic: credit and incentives.

\subsubsection{First breakout discussion: Group A}
The first sub-group discussed issues around the current system for credit and
current incentives, which it called ``hacking the incentive structure.'' The
group considered four potential points of leverage:

First, that we currently have systems that collect information, and these could
be modified to collect different information, then map that information to
actions. We could initially build a proof-of-concept for a new use of a given
system, then determine what actions would be needed to make this use more
common.

Second that we could create entirely new systems, perhaps because the existing
systems are too tied to what they measure, and modifying them is not practical.

Third that we could change academic culture, rather than worrying about the
systems. This was mostly focused on citations, because they matter for hiring,
promotion, and tenure decisions. The group discussed how we could weigh the
citations within papers better than we now do? How could we identify the five
citations that really matter for a paper, distinguishing them from the related
works and general background that are also cited. Perhaps we could break these
out in the reference list, working with publishers to implement this. Or maybe
we could also break out categories of citations, such as the most important
software used, the previous publication that we are building from, the data that
we actually used, etc. The Moore Foundation's award in data science was given as
an example, asking proposers: What are the five canonical citations that are
most important to your work?~\cite{moore-canonical} This would be a way of
giving credit and assigning importance to these works, differently from how we
just count citations today. A possible action that the group discussed was
conducting a longitudinal study of
most useful \{software, data, etc.\} in a discipline.

Fourth, that we could change the ways funders make decisions, and use these
funding policies as incentives.

After this discussion, group A brainstormed about incentives,
with the following items suggested:
\begin{itemize}
\item running programming contests, creating bounties for contributing to open
source software, etc.
\item augment author lists to give credit to people who do not now get credit
(and making them machine readable)
\item developing a microcitation standard and mechanism (for both software and
data)
\item developing a well-defined standard for author contribution -- what level
of contribution rises to the level of authorship?
\item leveraging social media for citation and reviewing of content -- then
using social media to bring more people into the review process than is
traditional
\item determining where else software can be cited and recorded (e.g.,
acknowledgments sections of papers)
\item developing a taxonomy of contributors (e.g., Project
Credit~\cite{projectcredit}) tied to places that these metrics are already
stored (e.g., ORCID~\cite{orcid})
\item making metadata easier to add for software, creating incentive for
providing software metadata -- note that this cannot be centralized
\item creating something like the h-index that tenure committees can make use of
- simplify a way of measuring and documenting the overall credit given to an
individual over different projects
\item thinking about publishing software versus journal - software does not have to
be novel
\item determining guidelines for recommending software characteristics for
tenure - perhaps draft guidelines then get ACM or IEEE agreement.
\end{itemize}


\subsubsection{First breakout discussion: Group B}

This group started by discussing who should be incentivized, thinking of two
categories of people: those in science (who could be incentivized to do better,
more shareable, more sustainable work), and those in industry but interested in
science (who could be incentivized to contribute to science.) It was pointed out
that we are not yet clear enough on exactly what we want to incentivize,
suggesting that we need to have a clearer picture of ``good computational work''
and what sort of contributions are truly generative for science.

The discussion of incentivizing those in science acknowledged that the
publications system was far from perfect for incentivizing good software work.
Nonetheless discussion focused on bringing software people into publications.
There were two main suggestions. The first is to focus on end users of software
and encourage them to cite software the ``right'' way. James Howison suggested
that his research showed that few projects were making a formal request for
citation (but that authors weren't necessarily following those suggestions
anyway)~\cite{howison2015jasist}. He suggested making access to the software
conditional on a license that requires citation. Others found this ``too
confrontational'' and preferred to concentrate on making it easier to do the
right thing. The second was focusing on those leading software projects, and the
group was more enthusiastic about ``forcing'' PIs to include their ``software
people'' on publications, although there were few ideas on how exactly to do
this. Another technique mentioned was that when scientific software projects are
hosted in organizations like Apache, the scientific contributors can benefit
from building their reputations, perhaps yielding job offers that they can use
to negotiate better job and career packages.

The discussion on incentivizing those outside science focused on accessing the
well of affection that those working in software have for scientific research.
How can the interest and skills of this group be marshaled towards sustainable
contributions? There is evidence that the migration of scientific software
projects to the Apache Foundation has created opportunities for those not
employed in the scientific center to contribute to projects initiated by
scientists (especially where there is cross-over with industry needs, such as
provenance and workflow).

The group also discussed developing ``software prizes'' arguing that while it is
hard to ``mint'' other new sources of reputation, prizes are possible without
getting too many others on board. The prize criteria can form a template for
describing what we mean by scientific contributions made through software,
particularly focusing on building active communities, not only writing great
code.

\subsubsection{Group merger and redivision}

After the first breakout session, the groups A and B came together and discussed
a rollup of the ideas from the subgroups at a high level:
\begin{itemize}
\item citation ecosystem - traces of usage (metrics)
\item taxonomy of contributorship, understanding roles
\item prizes
\item new metrics (for people's activities in software)
\item guidelines for evaluating scientific contribution through software (perhaps 
using new metrics).
\end{itemize}

In the remaining discussion sessions, the group chose to split into three
subgroups to discuss a version of these topics: {\em citation ecosystems}, {\em
taxonomy of contributorship/guidelines for software for tenure review}, and {\em
prizes}. The subgroups were asked to clearly identify
\begin{itemize}
\item the problem to be solved
\item steps towards a solution.
\end{itemize}



\subsubsection{Remaining breakout discussions: Citation ecosystems}

\note{for reference, notes are in \url{http://tinyurl.com/l4tc6a4}}
%https://docs.google.com/document/d/1SOja7k0mXwvOB2tQD3ZW3O38Qv4NwfA21zdMQ9JlUQc/edit

This group defined its goal as creating a low-barrier-of-entry method for
recording names and roles (and in a second phase, optionally including weights)
of contributors (coders and other intellectual contributions) to a software
package in a machine readable way (to be called a credit file), then encouraging
the scientific community to adopt this practice.

The following general points were initially discussed: 
\begin{itemize}
\item The FLASH~\cite{flash} project was suggested as an example of how something 
like this has been done.
\item This data could be a file that can be associated with a citation to the 
software, either through use of a DOI for the credit file, or by uploading the 
credit files as associated with the paper.
\item This idea could also be applied to data
\item This data (the credit file) should be part of the metadata that is outside
of paywalls, like citations are now. \choinote{what are ``paywalls''?}
\item It was suggested that there should also be a separate file to track
software dependencies, but this is not the same file. 
\end{itemize}

The group came up with the following actions to be performed:
\begin{enumerate}
\item Build a tool that can automatically determine who the contributors are 
(from a Git or other repository), then allows the user to manually edit the output to 
add/remove people, define roles. 
\item Work with repositories to encourage them to provide the information we 
need based on what they already store.
\item Define what a citation file should look like and what it should be called.
\item Test adoption, for example, with LBNL.
\item Create credit file for a set of software.
\item Build a validator (and perhaps a visualizer) for credit files.
\item Write a tool to collect files and visualize/output interconnections (which 
software is used with which), based on an existing project.
\item When we (the group members) write papers, we should track the software 
we use, and encourage the software developers to make their software citable 
and create credit files.
\item Build a tool to export the credit file to BibTex and other citation styles.
\item Make sure the BibTex entries (exported from internal data) are somewhat 
standardized so that they can be imported into papers.  Also make sure that 
standard LaTeX style files understand and accept these entries.
\end{enumerate}


\subsubsection{Remaining breakout discussions: Taxonomy of contributorship/guidelines for software for tenure review}
\note{by Frank Seinstra}

\note{for reference, notes are in \url{http://tinyurl.com/mx58tmx}}
%https://docs.google.com/document/d/1sDZLaBR2elZ9Tj6eLIwEtf5VqQ_x_AN_SetKtJlFUpI/edit

At the start of the discussion, the breakout group brought forth the important
observation of the wide disparity in commonly accepted habits of publication in
different research fields. In domains which have, historically, relied on large
groups of researchers collaborating towards a common goal (e.g., high-energy
physics, astronomy), publications often have tens or even hundreds of co-authors
(with some papers in experimental particle physics having over 3000.) In other
domains, the number of co-authors is typically much smaller, with, in some
cases, even a preference for single-author papers. Similarly, the various
platforms for publication are valued differently in different domains. Most
commonly, publications in peer-reviewed scientific journals are regarded as the
most important and most impactful. However, in certain domains, especially in
Computer Science, many researchers typically regard conference proceedings as
their prime publication target. It is often suggested that this difference is
due to the rapid developments in ICT science, a pace that can not be upheld by
traditional peer-reviewed journals. Whatever the causes, any useful taxonomy of
contributorship or guideline for tenure review should take such differences into
account.

Despite these differences, and despite the fact that software often has taken
the role of a proper, albeit less tangible, scientific research instrument,
neither the software nor its creators are commonly credited as part of a
scientific publication. The group acknowledged the need for more recognition for
the creators of such software instruments, and indicated a number of possible
pathways. First and foremost, domain scientists must be made aware of the
important role of software, and include the developers as co-authors of papers.
A second approach is to fully embrace an open badging infrastructure (such as
Mozilla's Open Badges), where a {\em badge\/} is a free, transferrable,
evidence-based indicator of an accomplishment, skill, quality, or interest. A
third approach is for the scientific community to support the increasing
momentum of peer-reviewed journals specialized in the open source/open access
publication of scientific research software, such as Computer Physics
Communication, F1000 Research, Journal of Open Research Software, and SoftwareX.

Recognizing publication of research software as a proper scientific contribution
raises several important but currently unsolved questions, however. For example,
is the number of users of the software a relevant measure of impact? What
standards of coding quality must be followed in order to justify publication and
hence recognition? Should the release of a new version of the software be
eligible for a new publication; if so, under what conditions? And above all:
should software publications be valued in the same way as traditional scientific
publications? Or is there a need for new measures of productivity and impact?

In part, the answers will come from the scientific community at large, as a
natural consequence of growing awareness and mindset change. Some of the
answers, however, also should be based on decades of experience in (and
developing standards for) implementing, maintaining, refactoring, documenting,
testing, and deploying software instruments in scientific research. Care should
be taken, however, not to impose such standards for all domains in equal ways
right from the start. Forerunners should serve as an example, but should not
scare away domains that have based their progress on much less advanced methods
of {\em software carpentry}. Nevertheless, proper guidelines are needed, which
eventually should be followed across all domains. The group also recognized that
funding bodies, universities, and publishers eventually should demand that
research projects follow such guidelines, and to implement a proper software
sustainability plan.

To enable a form of standardized crediting for developers of research software,
the group proposed to work towards a taxonomy for software-based
contributorship. The taxonomy should be derived from, or extend, existing
taxonomies for research impact and contributorship such as defined by CASRAI (in
particular based on the Wellcome-Harvard contributorship
taxonomy\footnote{Project CRediT, \url{http://credit.casrai.org/}}, VIVO, or
ISNI. An interesting measure of impact raised by the group was the {\em
betweenness centrality}, an indicator of a person's centrality (and hence,
importance) in a scientific collaboration. It is expected that developers of
research software often play such a central role. %\choinote{cite VIVO or ISNI?}

The group defined the following actions to be performed:
\begin{enumerate}
\item Investigate existing taxonomies for roles and contributorships.
\item Investigate prototype badging initiatives.
\item Investigate journals focusing on publishing peer-reviewed research
software.
\item Investigate guidelines and checklists of best practices.
\item Communicate the results of the above investigations to the WSSSPE
community, and to decision-making bodies (funders, publishers, universities,
and tenure committee representatives).
\item Ensure engagement of the broader research community in this discussion.
\end{enumerate}



\subsubsection{Remaining breakout discussions: Prizes}\label{sec:prizes}
\note{sub-breakout notes: \url{http://tinyurl.com/mjok4o5}}
%https://docs.google.com/document/d/1foSppeHQeQ-qOjd-Rp_CUCNy8JMLxd_vrCDMmqvG2-w/edit 

This group discussed the idea of prizes. Prizes are expected to reframe software
as ``instrument building'' but will prizes be good or bad, and how can we make
sure there are no negative affects and the process cannot be gamed?
  
Prizes in different categories were discussed (like Academy Awards), for
example: best contribution (non-founder), broadest diversity of contributions,
best tutorials or documentation, best leadership transition (award ex leader and
new leader), best generalization (taking something that was limited and making
it more general), and best mentorship of contributors (bringing others into the
community).
 
Other (non-prize) forms of incentives are
\begin{itemize}
\item Converting reputation by joining the Apache Software Foundation, Google, etc. 
\item Inviting ASF and open source people to contribute to scientific code (at
least on questions of overlapping
\item Template for assessing scientific contributions made through software.   
\end{itemize}

The group suggested that to determine who should give out prizes, perhaps we
should find those who we think would be awarded prizes, then ask them who they
would want to receive a prize from, then ask those organizations to see if they
are willing to be involved in the process.

One of the group's ideas was to create a funding program for disciplines or
other organizations to create a prize program. We would provide a framework and
a set of requirements, for example: awards to individuals; must award in 5 or 6
areas; must have a jury and/or objective criteria that includes senior/junior
domain people and technology people; must have the recipients awarded at a
relevant event; and must provide citations that explain why the prizes should be
awarded. Different organizations could then decide to sign up to the framework
and give out awards under this general brand. However, there was a concern that
having many organizations award prizes may reduce the impact of the prizes.

Possible groups that might give out awards, either under our framework, or more generally,
are AAAS, Nature Publishing Group, ACM/IEEE, Astro, Ecology
Society of America, etc. Perhaps this could be a joint technology/science
partnership, for example, the [Apache $|$ Mozilla]--[AAAS $|$ disciplinary
society] prize.

Some potential criteria for prizes the group suggested are:
community engagement, helping out others;
number of unique contributors;
adding new pieces of functionality to software;
integrating software into broader ecosystem; championing broad principles of
sustainability, open science, open source, etc.;
improving accessibility to software, to scientific software (perhaps championing
inclusiveness or making software accessible?);
documentation;
tutorials;
commits / patching;
leadership transition; and
best contribution by a non-founder.

The group discussion if there should be different criteria for ``established''
members of the community versus junior members, and if prizes should be
restricted to junior members, but left these as open questions.

A point the group considered important is that we do not want to give prizes just
to reward people who are really good at this one thing, but rather we want to
reward people who are building the culture we want as scientists.

 



%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Papers}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The papers that were discussed in the Citation \& Incentives group are:
\begin{itemize}
\item James Howison. Retract bit-rotten publications: Aligning incentives for
sustaining scientific software~\cite{wssspe2_howison}

\item Daniel S. Katz and Arfon M. Smith. Implementing Transitive Credit with
{JSON-LD}~\cite{wssspe2_katz}

\item Ian Kelley. Publish or perish: the credit deficit to making software and
generating data~\cite{wssspe2_kelley}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reproducibility \& Reuse \& Sharing} \label{sec:reproduce}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{Neil OR VOLUNTEER to lead writing of this section}

\note{\href{http://tinyurl.com/kqpe87z}{Google doc notes}}
%https://docs.google.com/document/d/1Mivadj16_9tPrw8Nxmhp72AtNC0BEVO-f7M84FXTbSU/edit#heading=h.elaktnt2tep9

This group discussed five papers with a wide variety ideas of how to support reproducibility and reuse. It focussed on identifying concrete that the attendees could work together on, which would have a positive effect on the community.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Discussion and Actions}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the first discussion session, this group broke up into two smaller groups working on different topics: Reproducibility, and Reuse and Sharing. In the second discussion session, three groups worked on creating specific pieces of guidance.

\subsubsection{First breakout discussion: Reproducibility group}

The first group discussed ways in which reproducibility of papers could be improved. A consensus surrounded the provision of examples: demonstrating to others how to achieve reproducibility. Major public funding investments go into research that heavily relies on software to be reproducible, hence lack thereof is raising major concerns.

Two main avenues could be used to implement policy that would improve reproducibility and drive top-down culture change: 
\begin{itemize}
\item Funders can aim to get more software expertise on to funding review panels and provide more guidance on what is required of software related to research (cf. data management plans)
\item Journals can define and publish journal policies to improve reproducibility, and reviewers can insist that authors provide sufficient information and access to data and software to allow them reproduce the results in the paper. Stronger policies even for some high-impact journals have recently come into place, for example Nature Publishing journals.
\end{itemize}

If journals do enforce greater reproducibility constraints, it is important to lower the barriers to reviewers attempting to verifying the correctness of the software used to generate the results. A major issue is that a lot of software only builds on certain systems. Should journals provide more tools / support for reviewers, and if so, what is it? An idea that came from the groups was to define a set of support services that should be available to software paper reviewers. Another was to provide the ability for reviewers to flag the requirement for a ?software verification?, similar to ability to flag that a paper needs to be seen by a statistician.

Other discussion focussed on ways in which researchers themselves could improve the reproducibility of research. One way would be to establish tracks at conferences that subject papers to reproduction, which for those that pass would lend them an additional badge (similar to OOPSLA). Another is simply to get more people to use your software: for instance by outreach to high school students - can your software be used by them? Finally, there is a role for community-curated benchmarks to validate the performance and capabilities of tools.

\subsubsection{First breakout discussion: Reuse and Sharing group}

The second group discussed ways in which software could be more easily reused and shared. From the perspective of both user and developer, any solutions must be 1) easy; 2) cheap; 3) not take much time.

Reuse and sharing were considered to be distinct but linked. In many cases, pre-existing software does not exist, so new software is written but even then it is not shared afterwards. The principal barriers to reuse are the difficulty of finding out what software is available, understanding if it is usable, and then of installing and running software if it is located.

Discovery of relevant software is still fundamental issue: we need standard vocabularies and metadata, better tools, and approaches that are sustainable. Publications are an easy entry point for locating suitable software, but should the publishers lead the way, or is this the responsibility of "the community"? Some communities have had significant initiatives to improve software discovery, e.g. the NIH Software Discovery Index\footnote{NIH Software Discovery Index: http://softwarediscoveryindex.org/}. Likewise, there were examples of journals which had made software more discoverable: ACM Transactions on Software offer a reproducibility review; the Journal of Biostatistics has an opt-in to provide code and a certification mark if it can be run; the Journal of Open Research Software requires software to be deposited in suitable repositories and referenced.

An issue around the sustainability of software catalogues is that their usefulness often depends on the domain. For instance, in the bio-sciences there is more homogenous data, and more standard shred codes. In areas like ecology, codes are very specific to the problem, meaning that the level of re-use might be at a very general statistical level of abstraction, but then every research use is highly customized.

Finally, it was clear from discussion that there were many ways in which software could be made available in more reusable ways that just a tarball sitting on a personal website. Using code repositories like GitHub\footnote{GitHub: http://github.com/} gets you an archived, shared platform and improve the reusability of your software incrementally, for instance by adding a licence or by archiving (with a DOI) in Figshare\footnote{FigShare: http://figshare.com/} or Zenodo\footnote{Zenodo: http://zenodo.org/}. Docker\footnote{Docker: htp://www.docker.com/} might be a solution to the issue of dependencies, to allow binaries and libraries to be bundled in a more lightweight fashion than a virtual machine image.

The key enabler for reuse and sharing was to get domain scientists more effectively connected with programmers/analysts. Both have skills and experience which is necessary to make the right decisions for improving the reusability and discoverability of software, and to apply community pressure to change practice.

\subsubsection{Second breakout discussion: Categorisation of software journals}

This group aimed to come up with a categorisation of journals which published software papers. Starting from the list of journals\footnote{http://www.software.ac.uk/resources/guides/which-journals-should-i-publish-my-software} maintained by the Software Sustainability Institute, the group chose seven journals and looked at their advice to authors and reviewers. These were the Journal of Open Research Software\footnote{http://openresearchsoftware.metajnl.com/about/editorialPolicies\#peerReviewProcess}, PLoS ONE\footnote{http://www.plosone.org/static/guidelines\#software}, Journal of Statistical Software\footnote{http://www.jstatsoft.org/instructions}. Methods in Ecology and Evolution\footnote{http://www.methodsinecologyandevolution.org/view/0/authorGuidelines.html}, Transactions of Mathematical Software\footnote{http://toms.acm.org/Authors.html}, GigaScience\footnote{http://www.gigasciencejournal.com/about}, and PLoS Computational Biology\footnote{http://www.ploscompbiol.org/static/guidelines\#software}. 

From these a set of common categories were synthesized, against which all journals could be compared:

\begin{itemize}
\item Journal Policies
\begin{itemize}
	\item Accessibility of papers
	\begin{itemize}
		\item Open Access
		\item ``Freely'' available
	\end{itemize}
	\item Repositories
	\begin{itemize}
		\item Provides suggestions for recommended repositories
		\item Provides its own repository
	\end{itemize}
	\item Review
	\begin{itemize}
		\item Reviewing software is mandatory
		\begin{itemize}
			\item Must check that software runs
			\item Must check quality of code
			\item Must check performance of code if paper makes claims on relative performance
		\end{itemize}
	\end{itemize}	
	\item Supporting Data
	\begin{itemize}
		\item Must be publicly available
		\item Must be in an Open Access repository
		\item must have a DOI
	\end{itemize}
	\item Article Processing Charge 
	\begin{itemize}
		\item APC charge is transparent
		\item APC waiver programme
	\end{itemize}
\end{itemize}

\item Paper Policies
\begin{itemize}
	\item Required Sections
	\item Keywords
	\begin{itemize}
		\item Paper provides keywords to help describe software
	\end{itemize}
	\item Papers can be updated when new releases of software are releases
	\begin{itemize}
		\item At no extra cost / at significantly reduced cost
	\end{itemize}
\end{itemize}

\item Software Policies
\begin{itemize}
	\item Software must have a license
	\begin{itemize}
		\item Software must have an open source license
	\end{itemize}
	\item Availability
	\begin{itemize}
		\item Software must be openly available and accessible
	\end{itemize}
	\item Deposit policies
	\begin{itemize}
		\item Software should be in a public repository
		\begin{itemize}
			\item Of particular stature / with preservation plan
		\end{itemize}
		\item Software should have a permanent identifier
		\item Software deposit doesn?t count as a prior publication
	\end{itemize}
	\item Runnability and dependencies
	\begin{itemize}
		\item Provide documentation to understand how to run
		\item Provide sample data
		\item Provide dependency information
	\end{itemize}
\end{itemize}
\end{itemize}

The follow-up actions to this work are to use this set of categories on all the journals in the list, refining the categories if necessary, then identify if any of the categories are seen to be more useful to promote reproducibility, reuse and sharing.

\subsubsection{Second breakout discussion: What should journals provide reviewers of software papers}

This group discussed whether they could come up with a list of things that a journal should provide its reviewers to make it easier to review software submitted for publication as a``Software Paper''.

Journals should provide guidelines about what to consider when reviewing a software submission. A good example for a relatively comprehensive guidelines for reviewers (and thus in turn authors) are those of JORS\footnote{http://openresearchsoftware.metajnl.com/about/editorialPolicies}. Journals might also learn from organisations such as the Apache Foundation as to what is good practice for software submissions. Guidance is needed on what constitutes an incremental improvement that is significant enough to qualify for publication, otherwise this assessment can be very subjective.

Journals should provide mechanisms to enable and track communication between reviewers and authors. For anonymous peer review journals, maintain anonymity of reviewers. If communication is necessary, that may mean that software is not that well documented. Journals should also provide a set of simple metrics for software evaluation that reviewers can use for ratings, similar to Consumer Reports.

Journals should provide guidelines about requirements for documentation of code: both in-lined in code, and manuals/web pages, etc. Journal editors could provide documentation checks before it goes out for review.  This should include a requirement for good Use Cases specified for the software, with references to executable test cases that demonstrate each use-case is met (at least in the form of the test case).

Journals should support mechanisms to run software. Sometimes this may be very hard to accomplish, despite best efforts. Journals could provide an Execution environment for any Software submitted for review, perhaps via a Docker container or a virtual machine. If not, instructions must be adequate to compile and execute the software; and interpret the results (output files, formats, etc.), and the full source code must be accessible to the reviewer. Mechanisms to quantify what has changed compared to a previously published version would assist version comparison. However, this cannot simply take lines of code into account. For example, a speedup of an algorithm may not result in a huge code difference, but may nonetheless provide a break-through. Journals should require software submissions to also provide ?Test materials?-- sample data, parameters to validate that code is working as intended. In certain cases, well-selected benchmark datasets may be required to assess performance and accuracy.

Journals should ensure minimal metadata are provided, similarly as is already the case for certain kinds of data (though the latter is often enforced by data repositories)-- Dublin core-ish (creator, owner), and more specific (platform and compiler dependencies, sample benchmarks of performance, etc.). Journals should provide guidance or constraints as to software licensing conditions.

The follow-up actions to this work are to work with journals and reviewers to identify whether any of these suggestions can be easily provided, perhaps for a range of journals.

\subsubsection{Second breakout discussion: Reproducibility Meta-track toolkit}

This group worked on defining a ``toolkit'' for running a reproducibility meta-track at a conference. They decided to take the work done during the workshop and publish it as a paper.



%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Papers}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The papers that were discussed in the Reproducibility \& Reuse \& Sharing group are:
\begin{itemize}
\item Jakob Blomer, Dario Berzano, Predrag Buncic, Ioannis Charalampidis,
Gerardo Ganis, George Lestaris and Ren\'{e} Meusel. The Need for a Versioned
Data Analysis Software Environment~\cite{wssspe2_blomer}

\item Ryan Chamberlain and Jennifer Schommer. Using {Docker} to Support
Reproducible Research~\cite{wssspe2_chamberlain}

\item Neil Chue Hong. Minimal information for reusable scientific
software~\cite{wssspe2_chue_hong}

\item Tom Crick, Benjamin A. Hall, and Samin Ishtiaq. ``Can I Implement Your
Algorithm?'': A Model for Reproducible Research Software~\cite{wssspe2_crick}

\item Bryan Marker, Don Batory, Field G. Van Zee, and Robert van de Geijn. Making
Scientific Computing Libraries Forward Compatible~\cite{wssspe2_marker}

\item Stephen Piccolo. Building Portable Analytical Environments to improve
sustainability of computational-analysis pipelines in the
sciences~\cite{wssspe2_piccolo}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Code Testing \& Code Review} \label{sec:code_testing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{Karen to lead writing of this section}

\note{\href{http://tinyurl.com/l5t5h45}{Google doc notes}}
%https://docs.google.com/document/d/1G1_U5-hkAvDKtTgT4DvUhIYvyYUY4cBBvQHJIdgADa0/edit#heading=h.v7g0nqpn1fur

\todo{short intro to group here}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Discussion and Actions}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Papers}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The papers that were discussed in the Code Testing \& Code Review group are:
\begin{itemize}
\item Thomas Clune, Michael Rilee, and Damian Rouson. Testing as an Essential
Process for Developing and Maintaining Scientific Software~\cite{wssspe2_clune}

\item Marian Petre and Greg Wilson. Code Review For and By
Scientists~\cite{wssspe2_petre}

\item Andrew E. Slaughter, Derek R. Gaston, John Peterson, Cody J. Permann,
David Andrs, and Jason M. Miller. Continuous Integration for Concurrent {MOOSE}
Framework and Application Development on {GitHub}~\cite{wssspe2_slaughter}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions} \label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo{currently just pre-workshop stuff - needs to be updated}

\katznote{add discussion of keynotes, lightning talks, charge to working groups about building plans, etc.}


\begin{table*}[t]
\centering
\caption{Top tweets tagged \#WSSSPE on Nov 16, 2014.}\label{tab:tweets}
  \begin{scriptsize}
  \begin{tabular}{l|l|r|r}
 \hline
    Author  &   Tweet  & Retweets &  Favorites
\\ \hline 
% Software Carpentry & Nov 17 & You call it ``project planning'' if it hasn't started yet,          & 8 & 1
%\\                          & &  and``software sustainability'' if it has and wasn't planned.     &    & 
%
% Neil P Chue Hong   &  Here's @SoftwareSaved guidance on Writing and using a software & 7 & 1
%\\     &  management plan used by EPSRC software grants    &    &  
% \\     &  http://w%ww.software.ac.uk/resources/guides/software-management-plans  &    & 
%
 Neil P Chue Hong  &  @jameshowison as well as software plans   & 4 & 7
\\   &   http://www.software.ac.uk/resources/guides/software-management-plans &    & 
\\   &   we provide a software evaluation tool:   &    & 
\\   &   http://www.software.ac.uk/online-sustainability-evaluation  &    & 
%
\\ Tom Crick  & $56\%$ of UK researchers develop their own software $\rightarrow  140,000$   &  14 & 8
\\ & UK researchers write research software w$/$out any formal training &    & 
%
\\ Karthik Ram  &  ``Institutionalize metadata before metadata institutionalizes you'' & 8 & 6
%
\\ Josh Greenberg  &  ``1. retract any paper with bitrotten dependencies'' *mic drop* & 13 & 8
\\   &   ``2. add anyone who fixes bitrot as an author'' *mic drop*  &    & 
%
\\ Ethan White &  ``@rOpenSci is all about community... our measures of success  & 9 & 3
\\ &   [include] how many faces are up on our community page''   &    & 
%
\\ Ethan White & Daniel Katz talking about implementing transitive credit for  & 9 & 7
\\ &  software http://arxiv.org/abs/1407.5117  Work with @arfon  &    & 
% 
%\\ Aleksandra Pawlik  &  ``Tell us how you test your scientific software'' @gvwilson @swcarpentry  & 7 & 2
% 
\\ Aleksandra Pawlik  & Lack of training as one of the main barriers for sustainable software & 10 & 4
\\ &    at @Supercomputing. @swcarpentry @datacarpentry can fix that!  & & 
%
\\ Kaitlin Thaney  & My slides from this morning's keynote at & 11 & 12
\\ &  WSSSPE on Designing for Truth, Scale $+$ Sustainability:  & & 
\\ &   http://www.slideshare.net/kaythaney/     & & 
\\ &  designing-for-truth-scale-and-sustainability-wssspe2-keynote   & & 
%
\\ Neil P Chue Hong & @kaythaney shout out for @swcarpentry @datacarpentry & 9 & 4
\\ &  @rOpenSci @stilettofiend around open training activities for sustainability  & & 
%
%\\ Richard Littauer &  Sweet! @swcarpentry has had 4000+ learners in the past year. & 6 & 
%
\\ Neil P Chue Hong & For those interested in Github - Figshare/Zenodo integration, & 5 & 12
\\ & but want SWORD/DSpace/Fedora/ePrints see:  & & 
\\ & http://blog.stuartlewis.com/2014/09/09/github-to-repository-deposit/  & & 
%
\\ Hilmar Lapp & Re: adopting the unix philosophy, consider signing the Small Tools in & 7 & 6
\\ & Bioinformatics Manifesto: https://github.com/pjotrp/bioinformatics   & 
%
\\Andre Luckow &  ``Traditions last not because they are excellent, & 12 & 3 
\\ & but because influential people are averse to change...''  C. Sunstein     & & 
% 
\\ Tom Crick &  ``Can I Implement Your Algorithm?'':  A Model for  Reproducible & 9 & 8
\\   &  Research Software http://arxiv.org/abs/1407.5981  & & 
%
\\Mozilla Science Lab & At a loose end this Sunday? Care about reproducibility, software  $+$ & 10 & 5
\\ &  \#openscience? Follow the    \#WSSSPE hashtag for more, live from New Orleans.  &  &
%
\\Kaitlin Thaney & I'm in New Orleans at \#WSSSPE , speaking at 9:50 ET on  scientific software & 9 & 9
\\ &   $+$ sustainability. Tune in! Live stream: http://ustre.am/17ddh & & 
%
\\   Daniel S. Katz & \#WSSSPE Agenda (Sunday):  & 10 & 1
\\ & http://wssspe.researchcomputing.org.uk/wssspe2/agenda/   &  &
\\ & URL for live stream of keynotes \& lightning talks: http://ustre.am/17ddh   &  &
\\ \hline
    \end{tabular}
    \end{scriptsize}
\end{table*} 


The WSSSPE2 workshop continues our experiment from WSSSPE1 in how we can
collaboratively build a workshop agenda. The differences in WSSSPE2 from WSSSPE1
are in using an existing service (EasyChair) to handle submissions and reviews,
rather than an ad hoc process, and using an existing service (well-sorted) to
allow collaborative grouping of papers into themes by all authors, reviewers,
and the community, rather than this being done in an ad hoc manner by the
organizers alone. 

The fact remains that contributors also want to get credit for their
participation in the process. And the workshop organizers still want to make
sure that the workshop content and their efforts are recorded. Ideally, there
would be a service that would be able to index the contributions to the
workshop, serving the authors, the organizers, and the larger community. But
since there still isn't such a service today, the workshop organizers are
writing this initial report and making use of arXiv as a partial solution to
provide a record of the workshop.

WSSSPE actively used the online social network Twitter, with hashtag
``\#WSSSPE''. There were substantially more tweets (messages) during the days of
the workshops WSSSPE2, WSSSPE1.1, and WSSSPE1. Out of about 670 tweets as of Apr
18, 2015, more than 225 were about WSSSPE2 and about 180 were posted during the
day of the workshop. Some of the main points and highlights in the meeting are
shown in Table~\ref{tab:tweets}, which summarizes the top \#WSSSPE tweets from
the day of workshop, selected by the metrics that number of retweets or
favourites larger than five and the sum of two measures greater than ten.

After the workshop, one or more additional papers will be created that will
include the discussions at the workshop. These papers will likely have many
authors, and may be submitted to peer-reviewed journals.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments} \label{sec:acks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{feel free to add stuff here}

Work by Katz was supported by the National Science Foundation while working at
the Foundation. Any opinion, finding, and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily
reflect the views of the National Science Foundation.


\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Attendees}  \label{sec:attendees}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following is a partial list of workshop attendees who registered on the
collaborative notes document~\cite{WSSSPE2-google-notes} that was used
for shared note-taking at the meeting, or who participated in a breakout groups
and were noted in that group's notes.


{\small
\begin{longtable}{ll}
   Jordan Adams          &  Tulane University
\\ Alice Allen           &  Astrophysics Source Code Library (ASCL)
\\ Gabrielle Allen       & University of Illinois Urbana-Champaign
\\ Pierre-Yves Aquilanti &  TOTAL E\&P R\&T USA
\\ Wolfgang Bangerth & Texas A\&M University
\\ David Bernholdt       &  Oak Ridge National Laboratory
\\ Jakob Blomer
\\ Carl Boettiger        &  University of California Santa Cruz \& rOpenSci
\\ Chris Bogart          &  ISR/CMU
\\ Steven R. Brandt      &  Louisiana State University
\\ Neil Chue Hong        &  Software Sustainability Institute \& University of Edinburgh
\\ Tom Clune             &  NASA GSFC
\\ John W. Cobb
\\ Dirk Colbry           &  Michigan State University
\\ Karen Cranston        &  NESCent
\\ Tom Crick             &  Cardiff Metropolitan University, UK
\\ Ethan Davis           &  UCAR Unidata
\\ Robert R Downs        &  CIESIN, Columbia University
\\ Anshu Dubey           &  Lawrence Berkeley National Laboratory
\\ Nicole Gasparini      &  Tulane University, New Orleans
\\ Yolanda Gil           &  Information Sciences Institute, University of Southern California
\\ Kurt Glaesemann       &  Pacific northwest national lab
\\ Sol Greenspan         &  National Science Foundation
\\ Ted Habermann         &  The HDF Group
\\ Marcus D. Hanwell     &  Kitware
\\ Sarah Harris          &  University of Leeds
\\ David Henty           &  EPCC, The University of Edinburgh
\\ James Howison         &  University of Texas
\\ Maxime Hughes
\\ Eric Hutton           &  University of Colorado
\\ Ray Idaszak           &  RENCI/UNC
\\ Samin Ishtiaq         &  Microsoft Research Cambridge, UK
\\ Matt Jones            &  University of California Santa Barbara
\\ Nick Jones            &  New Zealand eScience Infrastructure, University of Auckland
\\ Daniel S. Katz        &  University of Chicago \& Argonne National Laboratory
\\ Ian Kelley
\\ Hilmar Lapp           &  National Evolutionary Synthesis Center (NESCent) \& Duke University
\\ Christopher Lenhardt
\\ Richard Littauer      &  MIT \& University of Saarland
\\ Frank L\"{o}ffler     &  Louisiana State University
\\ Andre Luckow          &  Rutgers
\\ Berkin Malkoc         &  Istanbul Technical University
\\ Kyle Marcus           &  University at Buffalo
\\ Bryan Marker          &  The University of Texas at Austin
\\ Suresh Marru          &  Indiana University
\\ Robert H. McDonald    &  Data to Insight Center/Libraries, Indiana University
\\ Rupert Nash
\\ Andy Nutter-Upham     &  Whitehead Institute
\\ Abani Patra           &  University at Buffalo
\\ Aleksandra Pawlik     &  Software Sustainability Institute
\\ Cody J. Permann       &  Idaho National Laboratory
\\ John W. Peterson      &  Idaho National Laboratory
\\ Benjamin Pharr        &  University of Mississippi
\\ Stephen Piccolo       &  Brigham Young University, Utah
\\ Marlon Pierce         &  Indiana University
\\ Ray Plante            &  NCSA, University of Illinois Urbana-Champaign
\\ Sushil Prasad         &  Georgia State University, Atlanta
\\ Karthik Ram           &  Berkeley Institute for Data Science, University of California Berkeley \& rOpenSci
\\ Mike Rilee            &  NASA/GSFC \& Rilee Systems Technologies
\\ Erin Robinson         &  Foundation for Earth Science
\\ Mark Schildhauer      &  NCEAS, Univ. California, Santa Barbara
\\ Jory Schossau         &  Michigan State University
\\ Frank Seinstra        &  Netherlands eScience Center
\\ James Shepherd        &  Rice University
\\ Justin Shi
\\ Ardita Shkurti        &  University of Nottingham
\\ Alan Simpson          &  EPCC, The University of Edinburgh
\\ Carol Song            &  Purdue University
\\ James Spencer         &  Imperial College London
\\ Tracy Teal            &  Data Carpentry
\\ Kaitlin Thaney        &  Mozilla Science Lab
\\ Matt Turk             &  NCSA, University of Illinois Urbana-Champaign
\\ Colin C. Venters      &  University of Huddersfield
\\ Nathan Weeks
\\ Ethan White           &  University of Florida/Utah State University
\\ Nancy Wilkins-Diehr   &  San Diego Supercomputer Center, University of California San Diego
\\ Greg Wilson           &  Software Carpentry
\end{longtable}
}

\bibliographystyle{plain}

\bibliography{wssspe}
\end{document}

